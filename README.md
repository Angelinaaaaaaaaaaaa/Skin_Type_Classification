# Skin_Type_Classification
CSE151A Project by Chengkai Yao, Minghan Wu, Yunjin(Grace) Zhu, Yulin Chen, Angelina Zhang, Yue Yin

## Data Exploration 
In the file `preprocessed.ipynb`, we conducted an initial exploration of the dataset. Each image is a 650x650 pixel colored image, meaning each image has three channels. We split the dataset into training, testing, and validation sets with an 8:1:1 ratio. Each image in the training set is encoded with a true label for skin type classification.

## Data Preprocessing
As detailed in `preprocessed.ipynb`, we observed that most faces are centered in the images, which could negatively impact the performance of certain Machine Learning algorithms, particularly logistic regression and SVM, as they rely heavily on pixel location. To address this, we used a pre-trained model to crop out the faces and then padded the images with a grey background, maintaining the 650x650 dimension and centering the faces. We normalized the dataset by dividing all pixel values by 255, which helps to speed up the optimization process during model training. These preprocessing steps are documented in `Milestone2.ipynb`.

Given each image's high dimensionality (650x650x3, over a million dimensions), we recognized the need for dimensionality reduction to improve the performance of simple ML algorithms like Logistic Regression and SVM. Therefore, we proposed using PCA (Principal Component Analysis) for dimensionality reduction. We plan to implement and test this approach in Week 3, as part of training and testing our baseline models.

### Why PCA and Baseline Models
Principal Component Analysis (PCA)
PCA is a statistical technique used for dimensionality reduction. In our case, the original images have a very high dimensionality due to their size (650x650 pixels with 3 color channels, resulting in over a million dimensions per image). High-dimensional data leads to increased computational complexity, overfitting, and difficulty in visualizing and understanding the data, also known as curse of dimensionality. By applying PCA, however, we can reduce the dimensionality of the dataset while preserving as much variability as possible. PCA transforms the original features into a new set of uncorrelated features (principal components), ordered by the amount of variance they explain in the data. This allows us to retain the most important information while discarding less significant details. Reducing the number of dimensions makes the data more manageable for machine learning algorithms, leading to faster training times and potentially better performance.
In `Milestone3.ipynb`, we applied PCA to reduce the dimensionality of the data to 30 principal components. This step transforms the high-dimensional images into a lower-dimensional space, making them more manageable for our baseline models.

## Baseline Models
We chose Logistic Regression and Support Vector Machine (SVM) as our baseline models for the following reasons:

### Logistic Regression
Logistic Regression is a simple yet effective linear model for classification problems. It estimates the probability that a given input belongs to a particular class. Despite its simplicity, Logistic Regression can perform surprisingly well on many classification tasks, especially when the features are appropriately scaled and relevant. In our case, we aim to classify skin type into three classes, making Logistic Regression a suitable choice.
Also because logistic regression is designed for binary classification, we are considering using softmax to enable multi-class classification. We performed feature scaling using standardization and tried to ensure accuracy and reliability. By creating and training this model for multi-class classification using the 'newton-cg' solver, and decreasing the risk of overfitting.  However, the result is still not satisfactory, with only 28.35% in prediction.

### Support Vector Machine (SVM)
SVM is a powerful and versatile classifier that can handle both linear and non-linear classification tasks. It works by finding the hyperplane that best separates the classes in the feature space. SVM is effective in high-dimensional spaces and is relatively memory efficient. Moreover, with different kernel functions, SVM models complex decision boundaries, making it suitable for our dataset's high-dimensional nature.
For this model, we specifically use RBF as our kernel type for multi-class classification using the "one-vs-rest" strategy. After the data training, it resulted in an accuracy of 49.81% in prediction.

Both Logistic Regression and SVM serve as baseline models that help us understand the dataset's characteristics and set a performance benchmark. Once we have evaluated these baseline models, we are planning to design a CNN model, which processes the input image to a particular layer and produces a new image. Because CNN can handle spatial invariance situations efficiently, also can reduce the number of parameters through parameter sharing, it would be the best model fitting for our dataset. We expected after designing the CNN model, the accuracy would result in a satisfactory percentage.


### Why is the Baseline Performance So Poor?
Logistic Regression Logistic Regression is a linear model that estimates the probability of a class by applying a linear function to the input data and mapping the output using a logistic function. This model optimizes the cross-entropy loss, attempting to find a local minimum of that cost function. It performs well for linearly separable problems but struggles with complex, non-linear relationships like those in image data. Even after applying PCA for dimensionality reduction, Logistic Regression may still face challenges because PCA does not always make the data linearly separable. The inherent non-linear relationships in skin type classification make it difficult for Logistic Regression to capture the necessary patterns. Furthermore, any imbalances or complex patterns in the dataset can further degrade its performance, leading to a low accuracy of 28.35%.
Support Vector Machine (SVM) SVM is a more robust classifier that aims to find the optimal hyperplane that maximizes the margin between different classes. Unlike Logistic Regression, SVM does not just try to optimize a cost function but rather focuses on maximizing the margin, thus providing a buffer zone for better handling uncertainty in the test data. By using the RBF kernel, SVM maps the input data to a higher-dimensional space where classes can be separated more effectively. This approach helps SVM better manage the non-linear relationships within the data. As a result, SVM performs better than Logistic Regression, achieving a moderate accuracy of 42.54%. The reason for this improvement is that SVM aims to separate the three classes from each other as much as possible by maximizing the margin, leaving more room for uncertainty when faced with new, unseen data.
Dataset Characteristics and Model Performance The dataset consists of images, where the pixels exhibit high correlation. This high correlation between pixels is a unique characteristic of image data and poses challenges for simpler models like Logistic Regression and even more complex models like SVM. While SVM handles these challenges better due to its margin-maximizing strategy, it still falls short of high accuracy. This indicates that the next model should not only be more sophisticated but should also be capable of capturing the spatial correlations in the data. Convolutional Neural Networks (CNNs) are particularly well-suited for this task as they are designed to exploit the local correlations in image data, making them a promising next step for improving classification accuracy.
