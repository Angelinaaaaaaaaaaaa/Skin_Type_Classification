# Skin_Type_Classification
CSE151A Project by Chengkai Yao, Minghan Wu, Yunjin(Grace) Zhu, Yulin Chen, Angelina Zhang, Yue Yin

## Data Exploration 
The first exploration of the dataset was done in the file `preprocessed.ipynb`. Every image is 3 channels (RGB), 650X650 pixels. We split the data into training, testing and validation sets with a ratio of 8:1:1. Each picture in the training set has a true label for skin type classification.

## Data Preprocessing
As per what we did on preprocessed.ipynb, we realized that most faces were at the center of pictures; this might not go well with some machine learning algorithms like logistic regression and SVM which rely more on pixel position. In order to achieve this, we utilized a ready model to cut off faces and then fill these images with gray color at the same time maintaining its original dimension of 650x650 as well as positioning it centrally. To speed up the optimization process during model training, all pixel values are divided by 255 so as to normalize our data set. These steps are described in `Milestone2.ipynb`.

For this reason, reduction of dimensionality was considered in order to enhance the performance of a basic ML algorithm like SVM and Logistic Regression given that each image has high dimensionality (650x650x3, over a million dimensions) that could be improved. Consequently, we suggested the use of Principal Component Analysis (PCA) as a means to reduce dimensionality. In Week 3, we will be implementing and testing this approach as part of our baseline models which form part of training and testing.

### Why PCA and Baseline Models
Principal Component Analysis (PCA)
PCA is defined as a statistical technique used to reduce dimensionality. In our case, due to their size, these original images have very high dimensionality (650x650 pixels with 3 color channels results in over a million dimensions per image). However, it results in computational complexity when data becomes high-dimensional; overfitting the model; difficulty visualizing or understanding the data known as the curse of dimensionality. Nevertheless, PCA helps to reduce the datasetâ€™s dimensionality but maintains the most possible variability. By PCA method initial characteristics are transformed into new uncorrelated ones called principal components arranged by variance explaining levels within the dataset. It retains maximum crucial information while discarding the least significant details. In addition, reducing its dimensionalities makes it easily handle data for machine learning algorithms, which leads to faster training times and better performance.
In `Milestone3.ipynb`, we applied PCA to reduce the dimensionality of the data to 30 principal components. This step transforms the high-dimensional images into a lower-dimensional space, making them more manageable for our baseline models.

## Baseline Models
We chose Logistic Regression and Support Vector Machine (SVM) as our baseline models for the following reasons:

### Logistic Regression
Logistic Regression is a simple but effective linear model for classification problems. It estimates the probability that a given input belongs to a particular class. Despite its simplicity, Logistic Regression can perform surprisingly well on many classification tasks, especially when the features are appropriately scaled and relevant. In our case, we aim to classify skin type into three classes, so that Logistic Regression is a suitable choice.
Also because logistic regression is designed for binary classification, we are considering using softmax to enable multi-class classification. We performed feature scaling using standardization and tried to ensure accuracy and reliability. By creating and training this model for multi-class classification with 'newton-cg' solver, we decrease the risk of overfitting.  However, the result is still not satisfactory, with only 28.35% in prediction.

### Support Vector Machine (SVM)
SVM is a powerful classifier that can handle both linear and non-linear classification tasks. It works by finding the hyperplane that best separates the classes in the feature space. SVM is effective in high-dimensional spaces and is relatively memory efficient. Moreover, with different kernel functions, SVM models complex decision boundaries, suitable for our dataset's high-dimensional nature.
For this model, we use RBF as our kernel type for multi-class classification using the "one-vs-rest" strategy. After the data training, it resulted in an accuracy of 49.81% in prediction.

Both Logistic Regression and SVM serve as baseline models, helping us understand the dataset's characteristics and set a performance benchmark. Once we have evaluated these baseline models, we are planning to design a CNN model, which processes the input image to a particular layer and produces a new image. Because CNN can handle spatial invariance situations efficiently, also can reduce the number of parameters through parameter sharing, it would be the best model fitting for our dataset. We expected after designing the CNN model, the accuracy would result in a satisfactory percentage.

### Why is the Baseline Performance So Poor?
Logistic Regression Logistic Regression is a linear model that estimates the probability of a class by applying a linear function to the input data and mapping the output using a logistic function. This model optimizes the cross-entropy loss, attempting to find a local minimum of that cost function. It performs well for linearly separable problems but struggles with complex, non-linear relationships like those in image data. Even after applying PCA for dimensionality reduction, Logistic Regression may still face challenges because PCA does not always make the data linearly separable. The inherent non-linear relationships in skin type classification make it difficult for Logistic Regression to capture the necessary patterns. Furthermore, any imbalances or complex patterns in the dataset can further degrade its performance, leading to a low accuracy of 28.35%.
Support Vector Machine (SVM) SVM is a more robust classifier that aims to find the optimal hyperplane that maximizes the margin between different classes. Unlike Logistic Regression, SVM does not just try to optimize a cost function but rather focuses on maximizing the margin, thus providing a buffer zone for better handling uncertainty in the test data. By using the RBF kernel, SVM maps the input data to a higher-dimensional space where classes can be separated more effectively. This approach helps SVM better manage the non-linear relationships within the data. As a result, SVM performs better than Logistic Regression, achieving a moderate accuracy of 42.54%. The reason for this improvement is that SVM aims to separate the three classes from each other as much as possible by maximizing the margin, leaving more room for uncertainty when faced with new, unseen data.
Dataset Characteristics and Model Performance The dataset consists of images, where the pixels exhibit high correlation. This high correlation between pixels is a unique characteristic of image data and poses challenges for simpler models like Logistic Regression and even more complex models like SVM. While SVM handles these challenges better due to its margin-maximizing strategy, it still falls short of high accuracy. This indicates that the next model should not only be more sophisticated but should also be capable of capturing the spatial correlations in the data. Convolutional Neural Networks (CNNs) are particularly well-suited for this task as they are designed to exploit the local correlations in image data, making them a promising next step for improving classification accuracy.
